# Housing Prices Submission
***

## 1. Importing the needed libraries
```{r eval=FALSE, Packages}
library(dplyr)
library(ggplot2) # Data visualization
library(VIM)  # Plots missing data
library(mice)
library(ggthemes)
library(scales) # Scaling yaxis
library(caret)
library(ggcorrplot)
library(grid)
library(gridExtra)
library(corrplot)
```

## 2. Importing data
```{r}
train <- read.csv('/Users/wafic/Downloads/data/House_Prices/train.csv', stringsAsFactors = F, na.strings=c("","NA"))
test <- read.csv('/Users/wafic/Downloads/data/House_Prices/test.csv', stringsAsFactors = F, na.strings=c("","NA"))

test$SalePrice <- NA
full <- rbind(train, test)
dim(full)
```

## 3. Exploring Numeric Variables

#### 3.1 Extracting a numerical variables dataframe
```{r}
full_numeric <- train[sapply(full, is.numeric)]
summary(full_numeric)
```

#### 3.2 Numeric variables with high correlation
```{r}
corr<-cor(as.matrix(full_numeric))
corr_mtx <- as.matrix(sort(corr[,'SalePrice'], decreasing = TRUE))

CorHigh <- names(which(apply(corr_mtx, 1, function(x) abs(x)>0.5)))
corr <- corr[CorHigh, CorHigh]
corrplot.mixed(corr, tl.col="black", tl.pos = "lt")
```

#### 3.3 Spread of the Response Variable
```{r}
ggplot(full, aes(x='', y=SalePrice))+
  geom_boxplot()+
  scale_y_continuous('House Prices',labels = dollar)+
  theme_minimal()
```

#### 3.4 Overall Quality in relationship with the Price
```{r}
ggplot(full, aes(x=factor(OverallQual), y=SalePrice))+
  geom_boxplot()+
  theme_minimal()+
  scale_y_continuous('House Prices',labels = dollar)
```


#### 3.5 Prices in relationship with the living area in every house
```{r}
ggplot(full, aes(x=GrLivArea, y=SalePrice))+
  geom_point()+
  theme_minimal()+
  scale_y_continuous('House Prices',labels = dollar)+
  geom_smooth(method='lm',formula=y~x)
```
We can see a clear positive relationship between houses living area and price.
There are few outliers especially those 2 points where size is huge and price
is relatively low

#### 3.6 Prices in relationship with Garage Size
```{r}
ggplot(train, aes(x=factor(GarageCars), y=SalePrice))+
  geom_boxplot()+
  theme_minimal()+
  scale_y_continuous('House Prices',labels = dollar)
```

#### 3.6 Prices in relationship with size of the basement
```{r}
ggplot(full, aes(x=TotalBsmtSF, y=SalePrice))+
  geom_point()+
  theme_minimal()+
  scale_y_continuous('House Prices',labels = dollar)+
  geom_smooth(method='lm',formula=y~x)
```



## 4. Imputing Missing Points

#### 4.1 Detecting Missing data points in every variable
```{r}
nulls <- function(x){
  sum(is.na(x))
}

sapply(full, nulls)
```

```{r}
aggr(full, col=c('navyblue','red'), numbers=TRUE, sortVars=TRUE, labels=names(train), cex.axis=.6, gap=4, ylab=c("Histogram of missing data","Pattern"))
```

Not all the above null variables are missing data points, lot of these factor
variables are not available for some houses and thus are written as NA
- In the first part of imputing missing data points I will replace these nulls
with "None"
- In the seond part I will impute continous variables using mice
- In the third part I will impute the factor variables with the levels that has 
the most occurance. Although it is a dirty approach to imputing missing
datapoints, the issue is that these are only 1 point missing and going beyond
this method will provide no added value when training the model

#### 4.2 Replacing NA with "None"" for factor variables
```{r}
full$Alley[is.na(full$Alley)] <- 'None'

full$FireplaceQu[is.na(full$FireplaceQu)] <- 'None'

full$Fence[is.na(full$Fence)] <- 'None'

full$MiscFeature[is.na(full$MiscFeature)] <- 'None'

full$PoolQC[is.na(full$PoolQC)] <- 'None'

full$GarageCond[is.na(full$GarageCond)] <- 'None'
full$GarageQual[is.na(full$GarageQual)] <- 'None'
full$GarageFinish[is.na(full$GarageFinish)] <- 'None'
full$GarageType[is.na(full$GarageType)] <- 'None'
full$GarageYrBlt[is.na(full$GarageYrBlt)] <- 0

full$BsmtFinType1[is.na(full$BsmtFinType1)] <- 'None'
full$BsmtFinType1[is.na(full$BsmtFinType1)] <- 'None'
full$BsmtFinType2[is.na(full$BsmtFinType2)] <- 'None'
full$BsmtExposure[is.na(full$BsmtExposure)] <- 'None'
full$BsmtCond[is.na(full$BsmtCond)] <- 'None'
full$BsmtQual[is.na(full$BsmtQual)] <- 'None'

full$MasVnrType[is.na(full$MasVnrType)] <- 'None'
```

#### 4.3 Replacing Continous Variables using mice
```{r}
tempdata <- mice(full[-81], m=2, maxit=50, meth='pmm',seed=500)

completedData <- complete(tempdata,1)
full$LotFrontage <- completedData$LotFrontage
full$MasVnrArea <- completedData$MasVnrArea
full$BsmtFinSF1 <- completedData$BsmtFinSF1
full$BsmtFinSF2 <- completedData$BsmtFinSF2
full$BsmtUnfSF <- completedData$BsmtUnfSF
full$TotalBsmtSF <- completedData$TotalBsmtSF
full$BsmtFullBath <- completedData$BsmtFullBath
full$BsmtHalfBath <- completedData$BsmtHalfBath
full$GarageCars <- completedData$GarageCars
full$GarageArea <- completedData$GarageArea
```

#### 4.4 Replacing Factor variables with level of highest occurance
```{r}
full$Electrical[is.na(full$Electrical)] <- 'SBrkr'
full$Utilities[is.na(full$Utilities)] <- 'AllPub'
full$MSZoning[is.na(full$MSZoning)] <- 'RL'
full$Exterior1st[is.na(full$Exterior1st)] <- 'VinylSd'
full$Exterior2nd[is.na(full$Exterior2nd)] <- 'VinylSd'
full$KitchenQual[is.na(full$KitchenQual)] <- 'TA'
full$Functional[is.na(full$Functional)] <- 'Typ'
full$SaleType[is.na(full$SaleType)] <- 'WD'
sapply(full, nulls)
```
We do not have nay null datapoints anymore in our dataset

## 5. Exploring Factor Variables
Here we will factor some numeric variables used above as they have levels, and
I will rename the levels for them to be more logical for the reader

#### 5.1 Factoring Overall Quality renaming levels
```{r}
full$OverallQual <- factor(full$OverallQual)

levels(full$OverallQual) <- list('Very Excellent' =10, 'Excellent'=9, 'Very Good'=8, 'Good'=7, 'Above Average'=6, 'Average'=5, 'Below Average'=4, 'Fair'=3, 'Poor'=2, 'Very Poor'=1)

table(full$OverallQual)
```

#### 5.2 Overall Condition renaming levels
```{r}
full$OverallCond <- factor(full$OverallCond)

levels(full$OverallCond) <- list('Very Excellent' =10, 'Excellent'=9, 'Very Good'=8, 'Good'=7, 'Above Average'=6, 'Average'=5, 'Below Average'=4, 'Fair'=3, 'Poor'=2, 'Very Poor'=1)

table(full$OverallCond)
```

#### 5.3 Total number of rooms in living area
```{r}
full$TotRmsAbvGrd <- factor(full$TotRmsAbvGrd)
table(full$TotRmsAbvGrd)
```

#### 5.4 Total number of bathrooms in living area
```{r}
full$FullBath <- factor(full$FullBath)
table(full$FullBath)
```


#### 5.5 Extracting factor variables
```{r}
full_factor <- train[sapply(full, is.character)]

prop_table <- function(x){
  prop.table(table(x))*100
}
sapply(full_factor, prop_table)
```
We can see lot of categorical variables with one dominant factor that takes
almost all the frequencies of a predictor which makes its useless. Looking into
the predictors which have some proper spread among the levels, I will plot some 
below.

#### 5.7 Prices spread and number of houses among neighborhoods
```{r echo = FALSE, message=FALSE, warning=FALSE, error=FALSE, fig.width=7, fig.height=4}
# Removing datapoint with null values in sales
ggplot(full[!is.na(full$SalePrice),], aes(x=reorder(Neighborhood, SalePrice, FUN = median), y=SalePrice))+
  geom_boxplot()+
  theme_classic()+
  scale_y_continuous('House Prices',labels = dollar)
```
The median price differs by neighborhood which is expected but what is to be 
notes is that there is some serious discrepancies in some neighborhoods.

#### 5.8 Number of houses per neighborhood
```{r echo = FALSE, message=FALSE, warning=FALSE, error=FALSE, fig.width=7, fig.height=4}
ggplot(full, aes(Neighborhood))+
  geom_bar()+
  theme_classic()
```

#### 5.9 Number of rooms in respect to house prices
```{r}
ggplot(full, aes(x=TotRmsAbvGrd, y=SalePrice))+
  geom_boxplot()+
  theme_classic()+
  scale_y_continuous('House Prices',labels = dollar)
```
The more rooms the higher the price, but we can see that when the number of 
rooms become crazy for example 8 rooms and above the price tends to be more 
disperse and less compact

#### 5.10 Number of bathrooms in respect to house prices
```{r}
ggplot(full, aes(x=FullBath, y=SalePrice))+
  geom_boxplot()+
  theme_classic()+
  scale_y_continuous('House Prices',labels = dollar)
```

#### 5.11 Number of houses built per year
```{r echo = FALSE, message=FALSE, warning=FALSE, error=FALSE, fig.width=7, fig.height=4}
ggplot(full, aes(factor(YearBuilt)))+
  geom_bar()+
  theme_classic()+
  theme(axis.text.x = element_text(angle = 90, hjust = 5))
```
Although the size of the dataset is small to generalize, but we can see the
years after WW2 had an increase number of homes, and then we can see it
increasing in the late 90s and with the boost in easly 2000 due to the housing
bubble

## 6. Engineering New Variables

#### 6.1 Feature 1: Getting the age of the property as of 2018
```{r}
full$PropAge <- as.numeric(format(as.Date(Sys.time()),format="%Y"))-full$YearBuilt

ggplot(full, aes(x=PropAge, y=SalePrice))+
  geom_point()+
  scale_y_continuous('House Prices',labels = dollar)+
  theme_minimal()
```
We can see some negative correlation as the house gets older it gets less 
expensive

#### 6.2 Feature 2: Assigning a variable Renovate to every property
```{r}
full$RenovAge <- as.numeric(format(as.Date(Sys.time()),format="%Y"))-full$YearRemodAdd

full$Renovate <- ifelse(full$RenovAge == full$PropAge, 'No', 'Yes')

ggplot(full[!is.na(full$SalePrice),], aes(x=PropAge, y=SalePrice, colour=factor(Renovate)))+
  geom_point()+
  scale_y_continuous('House Prices',labels = dollar)+
  theme_minimal()
```
Mainly all houses above 70 years has been renovated

```{r}
ggplot(full, aes(x='', y=SalePrice))+
  geom_boxplot()+
  theme_minimal()+
  scale_y_continuous('House Prices',labels = dollar)+
  theme_minimal()+
  facet_grid(. ~ Renovate)
```
Since all houses above 70 has been renovated and those below 70 mostly havent
been, there is slight difference in the median price


#### 6.3 Feature 3: Total House Area in Square Feet
```{r}
full$TotalSqFeet <- full$GrLivArea + full$TotalBsmtSF

ggplot(full, aes(x='', y=TotalSqFeet))+
  geom_boxplot()
```


```{r}
ggplot(full[!is.na(full$SalePrice),], aes(x=TotalSqFeet, y=SalePrice))+
  geom_point()+
  geom_smooth(method='lm',formula=y~x)
```
Similar to the living area, when we added the total basement we can still see 
a clear linear relationship betwen the size of the house and its price. But 
looking at size vs price is a not how to evaluate the price of a house. Usually
it is the price per SQFT which is a better indicator

#### 6.4 Feature 4: Price per sqft of total square feet area
```{r}
full$PriceSQFT <- round(full$SalePrice / full$TotalSqFeet, 1)

ggplot(full[!is.na(full$SalePrice),], aes(x=TotalSqFeet, y=PriceSQFT))+
  geom_point()+
  scale_y_continuous('Price per SQFT',labels = dollar)
```
Unlike Salesprice, Price per SQFT is no linear relationship with TotalSqfoot, 
which is a great way of the data telling us that there are lots of other 
variables that will eventually affect the price


## 7. Some More Plots with Price per SQFT
Before running any ML algorithm, I want to know what is a main driver of price,
Because obvoiusly it aint size as we saw in price per SQFT. 

#### 7.1 Neighborhood with age and renovate
```{r echo = FALSE, message=FALSE, warning=FALSE, error=FALSE, fig.width=7, fig.height=4}
ggplot(full[!is.na(full$SalePrice),], aes(x=PropAge, y=PriceSQFT, colour=factor(Renovate)))+
  geom_point()+
  scale_y_continuous('Price per SQFT',labels = dollar)+
  geom_hline(yintercept=median(full[!is.na(full$SalePrice),]$PriceSQFT), linetype="dashed", color = "red")+
  facet_wrap(~Neighborhood)
```
Looking at these 2 plots, I am not sure if renovating your house will usually
increase its price because if we look at the green dots we see that they are
almost all below the median Price per SQTF. So if we look at the old town, we
see all houses are renovated and below median price, while on the other hand
Crawfor houses are renovated and most above the median price. 
Looks like neighborhood is an important predictor of price

#### 7.2 Quality vs Price per SQFT
```{r}
ggplot(full[!is.na(full$SalePrice),], aes(x=TotalSqFeet, y=PriceSQFT))+
  geom_point()+
  scale_y_continuous('Price per SQFT',labels = dollar)+
  geom_hline(yintercept=median(full[!is.na(full$SalePrice),]$PriceSQFT), linetype="dashed", color = "red")+
  facet_wrap(~OverallCond)
```
The majority of houses range between good and average but in general we can say 
that quality affect price per SQFT because for lower quality building, we can 
see that the price almost always is below median

#### 7.3 Kitchen Quality vs Price per SQFT
```{r}
ggplot(full[!is.na(full$SalePrice),], aes(x=TotalSqFeet, y=PriceSQFT))+
  geom_point()+
  scale_y_continuous('Price per SQFT',labels = dollar)+
  geom_hline(yintercept=median(full[!is.na(full$SalePrice),]$PriceSQFT), linetype="dashed", color = "red")+
  facet_wrap(~KitchenQual)
```
Looks like the quality of the kitchen can be an important predictor because
for Typical/Average and faid kitchen quality, the prices are below mostly below
median price per SQFT

## 8. Preprocessing Predictor Variables

#### 8.1 Removing Outliers
```{r}
full[!is.na(full$SalePrice),][full[!is.na(full$SalePrice),]$TotalSqFeet > 7500,]
```
These are very huge houses with cheap prices detected in above plots

```{r}
# Remove the above from the dataset
full <- full[-c(524, 1299),]
dim(full)
```

#### 8.2 Creating Dummy Variables for Regression Model
```{r}
dummies <- dummyVars(~., data = full)
full <- data.frame(predict(dummies, newdata = full))
dim(full)
```
Since, it is mostly a regression model that will be implmented to predict prices
of houses, we created dummy variables for all these categorical variables to be 
used in our model which ended up with 344 variable

#### 8.3 Zero- and Near Zero-Variance Predictors
```{r}
nzv <- nearZeroVar(full, saveMetrics= T)
nzv[ nzv[,"nzv"] > 0, ]
```
We can see only 1  zero variance predictors and 201 are near zero which implies
that lots of these dummy variables have very large frequency ratio and 
highly-unbalanced data and the percent of unique values approaches zero meaning
that these predictors provide little value for predicting the price.


```{r}
y <- nzv[ nzv[,"zeroVar"],]
drops<-names(data.frame(t(y)))

full<- full[ , !(names(full) %in% drops)]
dim(full)
```
Drop the zero variance variable

```{r}
x <- nzv[nzv[,"nzv"] + nzv[,"freqRatio"] > 30,]
drops<-names(data.frame(t(x)))
drops
```
We will drop 175 feature that are of frequency ratio above 30

```{r}
full<- full[ , !(names(full) %in% drops)]
dim(full)
```
After removing the 175 features we have 168 features

#### 8.4 Remove highly correlated variables
```{r}
correlationMatrix <- round(cor(full[-c(162, 168)]),1)
highlyCorrelated <- findCorrelation(correlationMatrix, cutoff=0.8)
head(full[highlyCorrelated])
```
These are the 26 predictors that are more than 80% correlated with other
predictors and which might cause multicollinearity in our model 

```{r}
full <- full[,-highlyCorrelated]
dim(full)

head(full)
```
Our model will try to predict house prices based on these 142 variables

#### 8.5 Scaling and Centering variables
```{r}
preprocessParams <- preProcess(full[-c(142, 138)], method=c("center", "scale"))
preprocessParams
```

```{r}
full_norm <- predict(preprocessParams, full)
dim(full_norm)
```

## 9. Model Training and Predictions

#### 9.1 Splitting the above data to train vs test datasets
```{r}
train_new <- full[!is.na(full$SalePrice),]
test_new <- full[is.na(full$SalePrice),]

dim(train_new);dim(test_new)
```

#### 9.2 Boosted Generalized Linear Model
```{r}
set.seed(1492)
### set training parameters
ctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 5)

### train the models
set.seed(300)

# Use the expand.grid to specify the search space  
glmBoostGrid = expand.grid(mstop = c(50, 100, 150, 200),
                               prune = c('yes', 'no'))


glmBoostFit <- train(SalePrice ~ ., 
              data = train_new, 
              method = "glmboost",
              metric = 'RMSE', 
              trControl = ctrl, 
              tuneGrid = glmBoostGrid)

glmBoostFit
```


```{r}
## Predict using the test data
plot(varImp(glmBoostFit), top = 10)

```



```{r}
set.seed(1492)
### set training parameters
ctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 5)

### train the models
set.seed(300)

# Use the expand.grid to specify the search space  
pcrGrid = expand.grid(ncomp = c(10, 40, 80, 140))

pcrFit <- train(SalePrice ~ ., 
              data = train_new, 
              method = "pcr",
              metric = 'RMSE', 
              trControl = ctrl,
              tuneGrid = pcrGrid)

pcrFit
```

```{r}
predict(pcrFit, test_new)
```


```{r}
set.seed(1492)
### set training parameters
ctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 5)

### train the models
set.seed(300)

# Use the expand.grid to specify the search space  
lmGrid = expand.grid(intercept = c(FALSE, TRUE))

lmFit <- train(SalePrice ~ ., 
              data = train_new, 
              method = "lm",
              metric = 'RMSE', 
              trControl = ctrl,
              tuneGrid = lmGrid)

lmFit
```
```{r}
predict(lmFit, test_new)
```

