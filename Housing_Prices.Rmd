# Housing Prices Submission
***

## 1. Importing the needed libraries
```{r eval=FALSE, Packages}
library(dplyr)
library(ggplot2) # Data visualization
library(VIM)  # Plots missing data
library(mice)
library(ggthemes)
library(scales) # Scaling yaxis
library(caret)
```

## 2. Importing data & basic EDA
```{r}
train <- read.csv('/Users/wafic/Downloads/data/House_Prices/train.csv', stringsAsFactors = F, na.strings=c("","NA"))
test <- read.csv('/Users/wafic/Downloads/data/House_Prices/test.csv', stringsAsFactors = F, na.strings=c("","NA"))
submission <- read.csv('/Users/wafic/Downloads/data/House_Prices/sample_submission.csv', stringsAsFactors = F, na.strings=c("","NA"))
```

```{r}
dim(train)
```


```{r}
str(train)
```

```{r}
ggplot(train, aes(x='', y=SalePrice))+
  geom_boxplot()+
  scale_y_continuous('House Prices',labels = dollar)+
  theme_minimal()
```

```{r}
train_factor <- train[sapply(train, is.character)]

prop_table <- function(x){
  prop.table(table(x))*100
}
sapply(train_factor, prop_table)
```
We can see lot of categorical variables with one dominant factor that takes
almost all the frequencies of a predictor which makes its useless

## 3. Feature Engineering & Selection

#### 3.1 Removing nulls
```{r}
nulls <- function(x){
  sum(is.na(x))
}

sapply(train, nulls)
```

```{r}
aggr(train, col=c('navyblue','red'), numbers=TRUE, sortVars=TRUE, labels=names(train), cex.axis=.6, gap=4, ylab=c("Histogram of missing data","Pattern"))
```


```{r}
train$Alley[is.na(train$Alley)] <- 'None'

train$FireplaceQu[is.na(train$FireplaceQu)] <- 'None'

train$Fence[is.na(train$Fence)] <- 'None'

train$MiscFeature[is.na(train$MiscFeature)] <- 'None'

train$PoolQC[is.na(train$PoolQC)] <- 'None'

train$GarageCond[is.na(train$GarageCond)] <- 'None'
train$GarageQual[is.na(train$GarageQual)] <- 'None'
train$GarageFinish[is.na(train$GarageFinish)] <- 'None'
train$GarageType[is.na(train$GarageType)] <- 'None'
train$GarageYrBlt[is.na(train$GarageYrBlt)] <- 0

train$BsmtFinType1[is.na(train$BsmtFinType1)] <- 'None'
train$BsmtFinType1[is.na(train$BsmtFinType1)] <- 'None'
train$BsmtFinType2[is.na(train$BsmtFinType2)] <- 'None'
train$BsmtExposure[is.na(train$BsmtExposure)] <- 'None'
train$BsmtCond[is.na(train$BsmtCond)] <- 'None'
train$BsmtQual[is.na(train$BsmtQual)] <- 'None'

train$MasVnrType[is.na(train$MasVnrType)] <- 'None'
```

```{r}
pmiss <- function(x){
  sum(is.na(x))/length(x)*100
}
pmiss(train$LotFrontage)
```

```{r}
tempdata <- mice(train, m=5, maxit=50, meth='pmm',seed=500)
tempdata$imp$LotFrontage
```


```{r}
completedData <- complete(tempdata,1)
train$LotFrontage <- completedData$LotFrontage
train$MasVnrArea <- completedData$MasVnrArea

train$Electrical <-factor(train$Electrical)
table(train$Electrical)
```

```{r}
train$Electrical[is.na(train$Electrical)] <- 'SBrkr'
```


```{r}
sapply(train, nulls)
```

```{r}
aggr(train, col=c('navyblue','red'), numbers=TRUE, sortVars=TRUE, labels=names(train), cex.axis=.6, gap=4, ylab=c("Histogram of missing data","Pattern"))
```

#### 3.2 Creating Dummy Variables for Regression Model
```{r}
dummies <- dummyVars(~., data = train)
train_dum <- data.frame(predict(dummies, newdata = train))
dim(train_dum)
```
Since, it is mostly a regression model that will be implmented to predict prices
of houses, we created dummy variables for all these categorical variables to be 
used in our model which ended up with 303 variables

#### 3.3 Zero- and Near Zero-Variance Predictors
```{r}
nzv <- nearZeroVar(train_dum, saveMetrics= TRUE)
nzv[ nzv[,"nzv"] > 0, ]
```
We cannot see any zero variance predictors but 176 are near zero which implies
that lots of these dummy variables have very large frequency ratio and 
highly-unbalanced data and the percent of unique values approaches zero meaning
that these predictors provide little value for predicting the price.

On the other hand I am very reluctant to remove them because as we see above, 
there are lots of houses with outlier prices and I am afraid these small
near zero variables are causing these differences. For example, if I look at 
the variables heatingfloor below, I see 1 in 1,460 observations but we know
that heating floor is a very expe

```{r}
print(table(train_dum$HeatingFloor))
print(train_dum[train_dum$HeatingFloor == 1,]$SalePrice)
```
I expected it to be an outlier but the house price was in our IQR range. I will
not keep all these 176 variables, nor I will drop them all. For this I will
choose variables with requency ratio above 

```{r}
# Remove the above 50 variables
x <- nzv[ nzv[,"nzv"] + nzv[,"freqRatio"] > 300,]
filteredDescr <- train_dum[, -x]
dim(filteredDescr)s
```

```{r}

```
After removing the 

#### 3.3 Remove highly correlated variables
```{r}
correlationMatrix <- cor(train_dum[-38])
highlyCorrelated <- findCorrelation(correlationMatrix, cutoff=0.8)
highlyCorrelated
```

#### 3.4 Centering and Scaling variables
```{r}

```


#### 3.3 Creating new features
```{r}

```

#### 3.5 Feature Transformation
```{r}

```


## 4 Variable importance

#### 5.2 Rank Features By Importance
```{r}
control <- trainControl(method="repeatedcv", number=10, repeats=3)
# train the model
model <- train(SalePrice~., data=train_numeric, method="glmboost", trControl=control)
# estimate variable importance
plot(varImp(model), top = 15)
```
