# Housing Prices Submission
***

## 1. Importing the needed libraries
```{r eval=FALSE, Packages}
library(dplyr)
library(ggplot2) # Data visualization
library(VIM)  # Plots missing data
library(mice)
library(ggthemes)
library(scales) # Scaling yaxis
library(caret)
library(ggcorrplot)
library(grid)
library(gridExtra)
library(corrplot)
```

## 2. Importing data
```{r}
train <- read.csv('/Users/wafic/Downloads/data/House_Prices/train.csv', stringsAsFactors = F, na.strings=c("","NA"))
test <- read.csv('/Users/wafic/Downloads/data/House_Prices/test.csv', stringsAsFactors = F, na.strings=c("","NA"))

test$SalePrice <- NA
full <- rbind(train, test)
dim(full)
```


#### 2.1 Overview of the variables available
```{r}
str(full)
```

#### 2.2 Factor variables
```{r}
full_factor <- train[sapply(full, is.character)]

prop_table <- function(x){
  prop.table(table(x))*100
}
sapply(full_factor, prop_table)
```
We can see lot of categorical variables with one dominant factor that takes
almost all the frequencies of a predictor which makes its useless

#### 2.4 Continous variables
```{r}
full_numeric <- train[sapply(full, is.numeric)]
summary(full_numeric)
```

## 3. EDA

#### 3.1 Measuring Spread of Prices
```{r}
ggplot(train, aes(x='', y=SalePrice))+
  geom_boxplot()+
  scale_y_continuous('House Prices',labels = dollar)+
  theme_minimal()
```
We can see some outliers but those two the top are way far from the rest,
Those are houses above $700,000.

#### 3.2 Numeric variables with high correlation
```{r}
corr<-cor(as.matrix(full_numeric))
corr_mtx <- as.matrix(sort(corr[,'SalePrice'], decreasing = TRUE))

CorHigh <- names(which(apply(corr_mtx, 1, function(x) abs(x)>0.5)))
corr <- corr[CorHigh, CorHigh]
corrplot.mixed(corr, tl.col="black", tl.pos = "lt")
```
The top three numeric variable that are correlated with SalePrice are the 
house Overall Quality, the Living Area and the size of Garage in terms of cars

#### 3.3 Overall Quality in relationship with the Price
```{r}
ggplot(train, aes(x=factor(OverallQual), y=SalePrice))+
  geom_boxplot()+
  theme_minimal()+
  scale_y_continuous('House Prices',labels = dollar)
```


#### 3.4 Prices in relationship with the living area in every house
```{r}
ggplot(train, aes(x=GrLivArea, y=SalePrice))+
  geom_point()+
  theme_minimal()+
  scale_y_continuous('House Prices',labels = dollar)+
  geom_smooth(method='lm',formula=y~x)
```

#### 3.5 Prices in relationship with Garage Size
```{r}
ggplot(train, aes(x=reorder(GarageCars, SalePrice, median), y=SalePrice))+
  geom_boxplot()+
  theme_minimal()+
  scale_y_continuous('House Prices',labels = dollar)
```









## 3. Data Cleaning from Nulls
```{r}
nulls <- function(x){
  sum(is.na(x))
}

sapply(train, nulls)
```

```{r}
aggr(train, col=c('navyblue','red'), numbers=TRUE, sortVars=TRUE, labels=names(train), cex.axis=.6, gap=4, ylab=c("Histogram of missing data","Pattern"))
```


```{r}
train$Alley[is.na(train$Alley)] <- 'None'

train$FireplaceQu[is.na(train$FireplaceQu)] <- 'None'

train$Fence[is.na(train$Fence)] <- 'None'

train$MiscFeature[is.na(train$MiscFeature)] <- 'None'

train$PoolQC[is.na(train$PoolQC)] <- 'None'

train$GarageCond[is.na(train$GarageCond)] <- 'None'
train$GarageQual[is.na(train$GarageQual)] <- 'None'
train$GarageFinish[is.na(train$GarageFinish)] <- 'None'
train$GarageType[is.na(train$GarageType)] <- 'None'
train$GarageYrBlt[is.na(train$GarageYrBlt)] <- 0

train$BsmtFinType1[is.na(train$BsmtFinType1)] <- 'None'
train$BsmtFinType1[is.na(train$BsmtFinType1)] <- 'None'
train$BsmtFinType2[is.na(train$BsmtFinType2)] <- 'None'
train$BsmtExposure[is.na(train$BsmtExposure)] <- 'None'
train$BsmtCond[is.na(train$BsmtCond)] <- 'None'
train$BsmtQual[is.na(train$BsmtQual)] <- 'None'

train$MasVnrType[is.na(train$MasVnrType)] <- 'None'
```

```{r}
pmiss <- function(x){
  sum(is.na(x))/length(x)*100
}
pmiss(train$LotFrontage)
```

```{r}
tempdata <- mice(train, m=5, maxit=50, meth='pmm',seed=500)
tempdata$imp$LotFrontage
```


```{r}
completedData <- complete(tempdata,1)
train$LotFrontage <- completedData$LotFrontage
train$MasVnrArea <- completedData$MasVnrArea

train$Electrical <-factor(train$Electrical)
table(train$Electrical)
```

```{r}
train$Electrical[is.na(train$Electrical)] <- 'SBrkr'
```


```{r}
sapply(train, nulls)
```

```{r}
aggr(train, col=c('navyblue','red'), numbers=TRUE, sortVars=TRUE, labels=names(train), cex.axis=.6, gap=4, ylab=c("Histogram of missing data","Pattern"))
```

## 4. REMOVEEEE    EDA (Trying to detect what variables has any relationship with Price)


#### 4.2 Prices spread among neighborhoods
```{r echo = FALSE, message=FALSE, warning=FALSE, error=FALSE,fig.width=7, fig.height=4}
ggplot(train, aes(x=reorder(Neighborhood, SalePrice, FUN = median), y=SalePrice))+
  geom_boxplot()+
  theme_classic()+
  scale_y_continuous('House Prices',labels = dollar)
```
The median price differs by neighborhood which is expected but what is to be 
notes is that there is some serious discrepancies in some neighborhoods.




## 5. Creating new features

#### 5.1 Feature 1: Getting the age of the property as of 2018
```{r}
train$PropAge <- as.numeric(format(as.Date(Sys.time()),format="%Y"))-train$YearBuilt

ggplot(train, aes(x=PropAge, y=SalePrice))+
  geom_point()+
  scale_y_continuous('House Prices',labels = dollar)+
  theme_minimal()
```


#### 5.2 Feature 2: Assigning a variable Reovate to every property
```{r}
train$RenovAge <- as.numeric(format(as.Date(Sys.time()),format="%Y"))-train$YearRemodAdd

train$Renovate <- ifelse(train$RenovAge == train$PropAge, 'No', 'Yes')

ggplot(train, aes(x=PropAge, y=SalePrice, colour=factor(Renovate)))+
  geom_point()+
  scale_y_continuous('House Prices',labels = dollar)+
  theme_minimal()
```
Mainly all houses above 70 years has been renovated

```{r}
ggplot(train, aes(x='', y=SalePrice))+
  geom_boxplot()+
  theme_minimal()+
  scale_y_continuous('House Prices',labels = dollar)+
  theme_minimal()+
  facet_grid(. ~ Renovate)
```
Since all houses above 70 has been renovated and those below 70 mostly havent
been, there is slight difference in the median price


#### 5.3 Feature 3: Garage Age
```{r}
train$GarageAge <- as.numeric(format(as.Date(Sys.time()),format="%Y"))-train$GarageYrBlt
train$GarageAge[train$GarageAge == 2018] <- 0
summary(train$GarageAge)
```

#### 5.4 Feature 4: Number of Years from when the property was Sold
```{r}
train$YearsSold <- as.numeric(format(as.Date(Sys.time()),format="%Y"))-train$YrSold
summary(train$YearsSold)
```

```{r}
ggplot(train, aes(x=YearsSold, y=SalePrice, fill=factor(YearsSold)))+
  geom_boxplot()
```
This variable looks of little affect on the price

#### 5.5 Feature 5: Price per sqft
```{r}
train$PriceSQFT <- round(train$SalePrice / train$LotArea, 1)
```

```{r warning=FALSE, error=FALSE,fig.width=7, fig.height=4}
ggplot(train, aes(x=reorder(Neighborhood, PriceSQFT, FUN = median), y=PriceSQFT))+
  geom_boxplot()+
  theme_classic()
```





## 6. Preprocessing

#### 6.1 Removing Outliers
```{r}
# These are very huge houses with cheap prices
train[train$GrLivArea > 4500,]
```

```{r}
# Remove them here
```


#### 6.2 Creating Dummy Variables for Regression Model
```{r}
dummies <- dummyVars(~., data = train)
train_dum <- data.frame(predict(dummies, newdata = train))
dim(train_dum)
```
Since, it is mostly a regression model that will be implmented to predict prices
of houses, we created dummy variables for all these categorical variables to be 
used in our model which ended up with 310 variables

#### 6.3 Zero- and Near Zero-Variance Predictors
```{r}
nzv <- nearZeroVar(train_dum, saveMetrics= T)
nzv[ nzv[,"nzv"] > 0, ]
```
We cannot see any zero variance predictors but 176 are near zero which implies
that lots of these dummy variables have very large frequency ratio and 
highly-unbalanced data and the percent of unique values approaches zero meaning
that these predictors provide little value for predicting the price.

On the other hand I am very reluctant to remove them because as we see above, 
there are lots of houses with outlier prices and I am afraid these small
near zero variables are causing these differences. For example, if I look at 
the variables heatingfloor below, I see 1 in 1,460 observations but we know
that heating floor is a very expe

```{r}
print(table(train_dum$HeatingFloor))
print(train_dum[train_dum$HeatingFloor == 1,]$SalePrice)
```
I expected it to be an outlier but the house price was in our IQR. I will
not keep all these 176 variables, nor I will drop them all. For this I will
choose variables with requency ratio above 300 to drop

```{r}
x <- nzv[ nzv[,"nzv"] + nzv[,"freqRatio"] > 300,]
drops<-names(data.frame(t(x)))
drops
```
We will drop 51 feature that are of frequency ratio above 300

```{r}
train_v1<- train_dum[ , !(names(train_dum) %in% drops)]
dim(train_dum);dim(train_v1)
```
After removing the 51 features we have 261 features


#### 6.4 Remove highly correlated variables
```{r}
correlationMatrix <- round(cor(train_v1[-253]),1)
highlyCorrelated <- findCorrelation(correlationMatrix, cutoff=0.8)
highlyCorrelated
```
These are predictors that are more than 80% correlated among themselves which 
makes them useless for our prediction

```{r}
train_v2 <- train_v1[,-highlyCorrelated]
dim(train_v2)
```

#### 6.5 Dropping a variable month sold
```{r}
train_v2 <- subset(train_v2, select=-c(MoSold, Id))
dim(train_v2)
```

> Before running any model for predictions, lets state the points we have so far
agreed on:
- Size of the house has little to do with the price
- Neighborhood has quite some positive effect on price
- 

## 7. Feature Scaling and Selection

#### 7.1 Centering and Scaling variables and PCA
```{r}
preprocessParams <- preProcess(train_v2[-221], method=c("center", "scale", "pca"))
# summarize transform parameters
print(preprocessParams)
# transform the dataset using the parameters
train_v3 <- predict(preprocessParams, train_v2[-221])
# summarize the transformed dataset
dim(train_v3)

train_data <- cbind(train_v3, train_v2[221])
```

## 6 Model Training and Predictions


