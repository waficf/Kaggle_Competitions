# Housing Prices Submission
***

## 1. Importing the needed libraries
```{r eval=FALSE, Packages}
library(dplyr)
library(ggplot2) # Data visualization
library(VIM)  # Plots missing data
library(mice)
library(ggthemes)
library(scales) # Scaling yaxis
library(caret)
library(ggcorrplot)
library(grid)
library(gridExtra)
```

## 2. Importing data & basic EDA
```{r}
train <- read.csv('/Users/wafic/Downloads/data/House_Prices/train.csv', stringsAsFactors = F, na.strings=c("","NA"))
test <- read.csv('/Users/wafic/Downloads/data/House_Prices/test.csv', stringsAsFactors = F, na.strings=c("","NA"))
submission <- read.csv('/Users/wafic/Downloads/data/House_Prices/sample_submission.csv', stringsAsFactors = F, na.strings=c("","NA"))
```

```{r}
dim(train)
```


```{r}
str(train)
```

```{r}
ggplot(train, aes(x='', y=SalePrice))+
  geom_boxplot()+
  scale_y_continuous('House Prices',labels = dollar)+
  theme_minimal()
```

```{r}
train_factor <- train[sapply(train, is.character)]

prop_table <- function(x){
  prop.table(table(x))*100
}
sapply(train_factor, prop_table)
```
We can see lot of categorical variables with one dominant factor that takes
almost all the frequencies of a predictor which makes its useless

## 3. Feature Engineering & Selection

#### 3.1 Removing nulls
```{r}
nulls <- function(x){
  sum(is.na(x))
}

sapply(train, nulls)
```

```{r}
aggr(train, col=c('navyblue','red'), numbers=TRUE, sortVars=TRUE, labels=names(train), cex.axis=.6, gap=4, ylab=c("Histogram of missing data","Pattern"))
```


```{r}
train$Alley[is.na(train$Alley)] <- 'None'

train$FireplaceQu[is.na(train$FireplaceQu)] <- 'None'

train$Fence[is.na(train$Fence)] <- 'None'

train$MiscFeature[is.na(train$MiscFeature)] <- 'None'

train$PoolQC[is.na(train$PoolQC)] <- 'None'

train$GarageCond[is.na(train$GarageCond)] <- 'None'
train$GarageQual[is.na(train$GarageQual)] <- 'None'
train$GarageFinish[is.na(train$GarageFinish)] <- 'None'
train$GarageType[is.na(train$GarageType)] <- 'None'
train$GarageYrBlt[is.na(train$GarageYrBlt)] <- 0

train$BsmtFinType1[is.na(train$BsmtFinType1)] <- 'None'
train$BsmtFinType1[is.na(train$BsmtFinType1)] <- 'None'
train$BsmtFinType2[is.na(train$BsmtFinType2)] <- 'None'
train$BsmtExposure[is.na(train$BsmtExposure)] <- 'None'
train$BsmtCond[is.na(train$BsmtCond)] <- 'None'
train$BsmtQual[is.na(train$BsmtQual)] <- 'None'

train$MasVnrType[is.na(train$MasVnrType)] <- 'None'
```

```{r}
pmiss <- function(x){
  sum(is.na(x))/length(x)*100
}
pmiss(train$LotFrontage)
```

```{r}
tempdata <- mice(train, m=5, maxit=50, meth='pmm',seed=500)
tempdata$imp$LotFrontage
```


```{r}
completedData <- complete(tempdata,1)
train$LotFrontage <- completedData$LotFrontage
train$MasVnrArea <- completedData$MasVnrArea

train$Electrical <-factor(train$Electrical)
table(train$Electrical)
```

```{r}
train$Electrical[is.na(train$Electrical)] <- 'SBrkr'
```


```{r}
sapply(train, nulls)
```

```{r}
aggr(train, col=c('navyblue','red'), numbers=TRUE, sortVars=TRUE, labels=names(train), cex.axis=.6, gap=4, ylab=c("Histogram of missing data","Pattern"))
```


#### 3.3 Creating new features

## 3.3.1 Getting the age of the property as of 2018
```{r}
train$PropAge <- as.numeric(format(as.Date(Sys.time()),format="%Y"))-train$YearBuilt

ggplot(train, aes(x=PropAge, y=SalePrice))+
  geom_point()+
  scale_y_continuous('House Prices',labels = dollar)+
  theme_minimal()
```


## 3.3.2 Assigning a varibale reovate to every property
```{r}
train$RenovAge <- as.numeric(format(as.Date(Sys.time()),format="%Y"))-train$YearRemodAdd

train$Renovate <- ifelse(train$RenovAge == train$PropAge, 'No', 'Yes')

ggplot(train, aes(x=PropAge, y=SalePrice, colour=factor(Renovate)))+
  geom_point()+
  scale_y_continuous('House Prices',labels = dollar)+
  theme_minimal()
```
Mainly all houses above 70 years has been renovated

```{r}
ggplot(train, aes(x='', y=SalePrice))+
  geom_boxplot()+
  theme_minimal()+
  scale_y_continuous('House Prices',labels = dollar)+
  theme_minimal()+
  facet_grid(. ~ Renovate)
```
Since all houses above 70 has been renovated and those below 70 mostly havent
been, there is slight difference in the median price

## 3.3.3 Putting houses in different sizes
```{r}
ggplot(train, aes(x='', y=LotArea))+
  geom_boxplot()+
  theme_minimal()
```
There seems to be very little discrepancy in the sizes of the property

```{r}
ggplot(train, aes(x=LotArea, y=SalePrice))+
  geom_point()+
  theme_minimal()+
  scale_y_continuous('House Prices',labels = dollar)
```
Very little number of huge houses but some are small and very expensive

```{r}
# divide houses into 3 groups of sizes
summary(train$LotArea)
```
```{r}
train$Size[train$LotArea < 12000] <- 'Normal'
train$Size[train$LotArea > 12000] <- 'Huge'
table(train$Size)
```

#### 

## 3.4.4 Putting houses in different Price ranges

## 3.4 Preprocessing

#### 3.4.1 Creating Dummy Variables for Regression Model
```{r}
dummies <- dummyVars(~., data = train)
train_dum <- data.frame(predict(dummies, newdata = train))
dim(train_dum)
```
Since, it is mostly a regression model that will be implmented to predict prices
of houses, we created dummy variables for all these categorical variables to be 
used in our model which ended up with 303 variables

#### 3.4.2 Zero- and Near Zero-Variance Predictors
```{r}
nzv <- nearZeroVar(train_dum, saveMetrics= T)
nzv[ nzv[,"nzv"] > 0, ]
```
We cannot see any zero variance predictors but 176 are near zero which implies
that lots of these dummy variables have very large frequency ratio and 
highly-unbalanced data and the percent of unique values approaches zero meaning
that these predictors provide little value for predicting the price.

On the other hand I am very reluctant to remove them because as we see above, 
there are lots of houses with outlier prices and I am afraid these small
near zero variables are causing these differences. For example, if I look at 
the variables heatingfloor below, I see 1 in 1,460 observations but we know
that heating floor is a very expe

```{r}
print(table(train_dum$HeatingFloor))
print(train_dum[train_dum$HeatingFloor == 1,]$SalePrice)
```
I expected it to be an outlier but the house price was in our IQR. I will
not keep all these 176 variables, nor I will drop them all. For this I will
choose variables with requency ratio above 300 to drop



```{r}
x <- nzv[ nzv[,"nzv"] + nzv[,"freqRatio"] > 300,]
drops<-names(data.frame(t(x)))
drops
```
We will drop 51 feature that are of frequency ratio above 300

```{r}
train_v1<- train_dum[ , !(names(train_dum) %in% drops)]
dim(train_dum);dim(train_v1)
```
After removing the 51 features we have 253 features

#### 3.4.3 Remove highly correlated variables
```{r}
correlationMatrix <- round(cor(train_v1[-253]),1)
highlyCorrelated <- findCorrelation(correlationMatrix, cutoff=0.8)
highlyCorrelated
```
These are predictors that are more than 80% correlated among themselves which 
makes them useless for our prediction

```{r}
train_v2 <- train_v1[,-highlyCorrelated]
dim(train_v2)
```











#### 3.5 Centering and Scaling variables
```{r}

```

#### 3.5 Feature Transformation
```{r}

```


## 4 Variable importance

#### 5.2 Rank Features By Importance
```{r}
control <- trainControl(method="repeatedcv", number=10, repeats=3)
# train the model
model <- train(SalePrice~., data=train_numeric, method="glmboost", trControl=control)
# estimate variable importance
plot(varImp(model), top = 15)
```
